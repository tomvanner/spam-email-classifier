{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naïve Bayes Classifier\n",
    "\n",
    "The training set consists of 1000 rows and 55 columns. Each row corresponds to one email message. The first column is the _response_ variable and describes whether a message is spam (1) or ham (0). The remaining 54 columns represent _feature_ _vectors_ that are used to train the classifier. These features correspond to 54 different keywords (such as \"money\", \"free\", and \"receive\") and special characters (such as \":\", \"!\", and \"$\"). A feature has the value \"1\" if the keyword appears in the message and \"0\" otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model\n",
    "The [naïve Bayes](https://en.wikipedia.org/wiki/Naive_Bayes_classifier) classifier distinguishes between two classes:\n",
    "\n",
    "* **$C = 1$ for spam messages **\n",
    "* **$C = 0$ for ham messages. **\n",
    "\n",
    "\n",
    "The classifier builds a model for the probability $P(C=c\\ |\\ message)$ that a given message belongs to a certain class. A new message is then classified based on the Bayesian *maximum a posteriori* estimate\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{c} = \\underset{c \\in \\{0,1\\}}{argmax} \\  P(C=c\\ |\\ message).\n",
    "\\end{equation}\n",
    "\n",
    "Using Bayes' rule we can write\n",
    "\n",
    "\\begin{equation}\n",
    "P(C=c\\ |\\ message) = \\frac{P(message\\ |\\ C=c)P(C=c)}{P(message\\ |\\ C=1)P(C=1) + P(message\\ |\\ C=0)P(C=0)}.  \\quad \\quad \n",
    "\\end{equation}\n",
    "\n",
    "The denominator is the same for both classes and we can thus drop it to get\n",
    "\n",
    "\\begin{equation}\n",
    "P(C=c\\ |\\ message) \\propto P(message\\ |\\ C=c)P(C=c).\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Messages are represented using a binary [bag-of-words](https://en.wikipedia.org/wiki/Bag-of-words_model) model. Specifically, a message is represented as $\\mathbf{w} = (w_1, ..., w_k)$, where $w_i = 1$ if the word $w_i$ appears in the message and $w_i = 0$ otherwise. We assume **class-conditional independence between occurences of known words** and can therefore write \n",
    "\n",
    "\\begin{equation}\n",
    "P(message\\ |\\ C=c) = \\prod_{i = 1}^k P(w_i\\ |\\ C=c).\n",
    "\\end{equation}\n",
    "\n",
    "The classifier now can be written as follows :\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{c} = \\underset{c \\in \\{0,1\\}}{argmax} \\ [ P(C=c)   \\prod_{i = 1}^k P(w_i\\ |\\ C=c) ].\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "### Multinomial Naïve Bayes\n",
    "\n",
    "Different naïve Bayes models differ in their distributional assumptions of $P(w\\ |\\ C=c)$, that is, the conditional likelihood of a word $w$ given a class $c$. We will model $P(w\\ |\\ C=c)$ using a [multinomial distribution](https://en.wikipedia.org/wiki/Multinomial_distribution). Intuitively, the multinomial distribution assumes that the words of a message are \"drawn\" independently from a bag of $k$ different keywords. Depending on the class membership $c$, each keyword has a probability $\\theta_{class, word}$ of being drawn. For example,\n",
    "\n",
    "* $\\theta_{spam, w}$ will have high value for $w \\in \\{$bank, transfer, buy... $\\}$.\n",
    "* $\\theta_{ham, w}$ will have high value for $w \\in \\{$paper, conference, proposal, experiment... $\\}$, if the training data was mostly gathered from emails of researchers.\n",
    "\n",
    "Both the class priors $P(C=c)$ and the class-conditional likelihoods $\\theta_{c, w} = P(w\\ |\\ C=c)$ have to be estimated from the training data. The parameters $\\theta_{c, w}$ are estimated by counting the relative frequencies in the training data. Use **Laplace-smoothing** with $\\alpha = 1$, that is,\n",
    "\n",
    "\\begin{equation}\n",
    "\\theta_{c, w} = \\frac{n_{c, w} + \\alpha}{n_{c} + k \\alpha},\n",
    "\\end{equation}\n",
    "\n",
    "where $n_{c, w}$ is the number of times the word $w$ appears in messages of class $c$ in the training set, $n_{c}$ is the total count of words for all messages of class $c$, and $k$ is the number of features (key-words).\n",
    "\n",
    "The likelihood of observing $\\mathbf{w}$ in a message of class $c$ is proportional to\n",
    "\\begin{equation}\n",
    "P(\\mathbf{w}\\ |\\ C=c) \\propto \\prod_{i = 1}^k  (\\theta_{c, i})^{w_i}.\n",
    "\\end{equation}\n",
    "\n",
    "### Increasing numerical stability\n",
    "The numerical result of the classifier will often lead to small numbers, which, due to floating point representation, can cause inaccuracies, and as such, the natural logorithm of the posteriror distributions should be taken instead\n",
    "\\begin{equation}\n",
    "\\hat{c} = \\underset{c \\in \\{0,1\\}}{argmax} \\ log( P(C=c)   \\prod_{i = 1}^k P(w_i\\ |\\ C=c) ) \\\\\n",
    " = \\underset{c \\in \\{0,1\\}}{argmax} \\ [ log( P(C=c)) + \\sum_{i = 1}^k w_i \\ log(\\theta_{c, i}) ].\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell imports the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "training_spam = np.loadtxt(open(\"data/training_spam.csv\"), delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to estimate the prior probability for each class, as to get $P(C=c)$ for $c \\in \\{0, 1\\}$ part of the classificiation equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_log_class_priors(data):\n",
    "    \"\"\"\n",
    "    Given a data set with binary response variable (0s and 1s) in the\n",
    "    left-most column, calculate the logarithm of the empirical class priors,\n",
    "    that is, the logarithm of the proportions of 0s and 1s:\n",
    "    log(P(C=0)) and log(P(C=1))\n",
    "\n",
    "    :param data: a two-dimensional numpy-array with shape = [n_samples, 1 + n_features]\n",
    "                 the first column contains the binary response (coded as 0s and 1s).\n",
    "\n",
    "    :return log_class_priors: a numpy array of length two\n",
    "    \"\"\"\n",
    "    #Finds the number of elements that are of class 0 and 1, and divides them by total number of samples\n",
    "    num_samples = data.shape[0]\n",
    "    actual_classes = data[:,0]\n",
    "    \n",
    "    log_class_priors = np.array([np.log(occurrences_in_column(actual_classes, 0) / num_samples), \n",
    "                                 np.log(occurrences_in_column(actual_classes, 1) / num_samples)])\n",
    "    \n",
    "    return log_class_priors\n",
    "\n",
    "def occurrences_in_column(col, needle):\n",
    "    \"\"\"\n",
    "    Given a one dimensional numpy array, count the number of occurences\n",
    "    of the given value\n",
    "\n",
    "    :param col: the data set to filter\n",
    "    :param needle: the value which to count the occurences of\n",
    "\n",
    "    :return: the number of occurences of the needle in the data set\n",
    "    \"\"\"\n",
    "    return col[col == needle].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to find the empirical class-conditional likelihoods $log(P(w_i | c_j))$ for all words $w_i$ and both classes ($j \\in {0, 1}$), where $\\theta_{c, w} = P(w\\ |\\ C=c)$. Since we are using laplace smoothing, we can say\n",
    "\\begin{equation}\n",
    "\\theta_{c, w} = \\frac{n_{c, w} + \\alpha}{n_{c} + k \\alpha},\n",
    "\\end{equation}\n",
    "where $\\alpha = 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_log_class_conditional_likelihoods(data, alpha=1.0):\n",
    "    \"\"\"\n",
    "    Given a data set with binary response variable (0s and 1s) in the\n",
    "    left-most column and binary features, calculate the empirical\n",
    "    class-conditional likelihoods, that is,\n",
    "    log(P(w_i | c_j)) for all features i and both classes (j in {0, 1}).\n",
    "\n",
    "    Assume a multinomial feature distribution and use Laplace smoothing\n",
    "    if alpha > 0.\n",
    "\n",
    "    :param data: a two-dimensional numpy-array with shape = [n_samples, n_features]\n",
    "\n",
    "    :return theta:\n",
    "        a numpy array of shape = [2, n_features]. theta[j, i] corresponds to the\n",
    "        logarithm of the probability of feature i appearing in a sample belonging \n",
    "        to class j.\n",
    "    \"\"\"\n",
    "    # Each message will have the same number of features (columns)\n",
    "    num_features= data.shape[1] - 1\n",
    "    \n",
    "    #Placeholder\n",
    "    theta = np.zeros((2,num_features))\n",
    "    \n",
    "    for _class in range(2):\n",
    "        #Gets a list of rows which are of the given class (0 or 1)\n",
    "        class_list = data[data[:,0] == _class]\n",
    "        \n",
    "        #Loops through each row\n",
    "        for feature_vector in class_list:\n",
    "            total_words = 0\n",
    "            \n",
    "            #Loops through each feature (column) in the vector, excluding the message identifier, i.e. index 0\n",
    "            for feature in range(1, num_features + 1):\n",
    "                # Gets the total number of present words for that feature\n",
    "                total_words += np.sum(class_list[:, feature])\n",
    "                \n",
    "                #Store the numerator of {theta_c,w} equation for each word, starting at index 0\n",
    "                theta[_class][feature-1] = np.log((np.sum(class_list[:, feature]) + alpha))\n",
    "                \n",
    "        #Finish the calculation, storing the class conditional likelihood for each class\n",
    "        theta[_class] = np.divide(theta[_class], np.log(total_words + num_features*alpha))\n",
    "\n",
    "    return theta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have gathered all of the constituent components of the classification equation from the training, we can now put the model to use by attempmting to classify messages in a data set using \\begin{equation}\n",
    "\\hat{c} = \\underset{c \\in \\{0,1\\}}{argmax} \\ [ log( P(C=c)) + \\sum_{i = 1}^k w_i \\ log(\\theta_{c, i}) ].\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(new_data, log_class_priors, log_class_conditional_likelihoods):\n",
    "    \"\"\"\n",
    "    Given a new data set with binary features, predict the corresponding\n",
    "    response for each instance (row) of the new_data set.\n",
    "\n",
    "    :param new_data: a two-dimensional numpy-array with shape = [n_test_samples, n_features].\n",
    "    :param log_class_priors: a numpy array of length 2.\n",
    "    :param log_class_conditional_likelihoods: a numpy array of shape = [2, n_features].\n",
    "        theta[j, i] corresponds to the logarithm of the probability of feature i appearing\n",
    "        in a sample belonging to class j.\n",
    "    :return class_predictions: a numpy array containing the class predictions for each row\n",
    "        of new_data.\n",
    "    \"\"\"\n",
    "    #Placeholder\n",
    "    class_predictions = np.zeros(new_data.shape[0])\n",
    "    \n",
    "    #Loop through each row of data (each sample)\n",
    "    for sample_num in range(new_data.shape[0]):\n",
    "        sample = new_data[sample_num]\n",
    "        max_likelihood = 0\n",
    "        \n",
    "        #loop through each class prior\n",
    "        for _class in range(log_class_priors.size):\n",
    "            likelihood = 0\n",
    "            \n",
    "            #Loop through each feature of the current sample\n",
    "            for feature in range(sample.size):\n",
    "                #Sum the conditional probabilities for each feature, i.e. the right most part of the equation\n",
    "                likelihood += sample[feature] * log_class_conditional_likelihoods[_class][feature]\n",
    "                \n",
    "            #Add class prior\n",
    "            class_likelihood = log_class_priors[_class] + likelihood\n",
    "            \n",
    "            #Finds the class which maximises the likelihood and store the result\n",
    "            if class_likelihood > max_likelihood:\n",
    "                max_likelihood = class_likelihood\n",
    "                class_predictions[sample_num] = _class\n",
    "                \n",
    "    return class_predictions\n",
    "\n",
    "def accuracy(y_predictions, y_true):\n",
    "    \"\"\"\n",
    "    Calculate the accuracy.\n",
    "    \n",
    "    :param y_predictions: a one-dimensional numpy array of predicted classes (0s and 1s).\n",
    "    :param y_true: a one-dimensional numpy array of true classes (0s and 1s).\n",
    "    \n",
    "    :return acc: a float between 0 and 1 \n",
    "    \"\"\"\n",
    "    #Comapres the predictions to actual classes\n",
    "    successes = np.equal(y_predictions, y_true)\n",
    "    \n",
    "    #Finds number of successful predictions\n",
    "    successes[successes[:] == True].size\n",
    "    acc = successes[successes[:] == True].size / successes.size\n",
    "    \n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier accuracy: 81.8%\n"
     ]
    }
   ],
   "source": [
    "# Train the classifier with out training data\n",
    "log_class_priors = estimate_log_class_priors(training_spam)\n",
    "log_class_conditional_likelihoods = estimate_log_class_conditional_likelihoods(training_spam, alpha=1.0)\n",
    "\n",
    "testing_spam = np.loadtxt(open(\"data/testing_spam.csv\"), delimiter=\",\")\n",
    "\n",
    "class_predictions = predict(testing_spam[:, 1:], log_class_priors, log_class_conditional_likelihoods)\n",
    "true_classes = testing_spam[:, 0]\n",
    "testing_set_accuracy = accuracy(class_predictions, true_classes)\n",
    "\n",
    "print(\"Classifier accuracy:\", str(testing_set_accuracy * 100) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naïve Bayes assumes that features are mutually exclusive. In the case of spam emails, this is unlikely to be the case, i.e. certain words are likely to appear in sequence. To resolve this, we can use other classification methods such as k-nearest neighbours and logistic regession \n",
    "\n",
    "Logistic regression is a light weight and relatively simple yet robust algorithm, which is able to classify unseen data points through the use of the logistic function, namely  $\\frac{1}{(1 + e^{-wx})}$, which gives the probabiltiy of the point belonging to class 1. For a point in n-dimensional space (n features), it can be mapped to a real number by multiplying its position vector by a series of weighting coefficients, i.e. $wx = w_1 x_1 + w_2 x_2 +...+w_n x_n$. The aim of the training phase is to calculate values for these coefficients which minimises the error rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(training_data, model):\n",
    "    \"\"\"\n",
    "    Train a model on the training_data\n",
    "\n",
    "    :param training_data: a two-dimensional numpy-array with shape = [5000, 39] \n",
    "    :param model: the sklearn model to use to classify the data \n",
    "    \n",
    "    :return fitted_model: any data structure that captures your model\n",
    "    \"\"\"  \n",
    "    #Extracts the features from training data\n",
    "    feature_data = training_data[:, 1:training_data.shape[1]]\n",
    "    #Gets the classes for each sample\n",
    "    classes = training_data[:,0]\n",
    "    \n",
    "    #Fits the data into the given model\n",
    "    fitted_model = model.fit(feature_data, classes)  \n",
    "    \n",
    "    return fitted_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(testing_data, fitted_model):\n",
    "    \"\"\"\n",
    "    Classify the rows of testing_data using a fitted_model. \n",
    "\n",
    "    :param testing_data: a two-dimensional numpy-array with shape = [n_test_samples, 38]\n",
    "    :param fitted_model: the output of your train function.\n",
    "\n",
    "    :return class_predictions: a numpy array containing the class predictions for each row\n",
    "        of testing_data.\n",
    "    \"\"\"\n",
    "    class_predictions = fitted_model.predict(testing_data)\n",
    "    return class_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross validation is used to get an estimate of the model's accuracy by training the model on $k-1$ partitions of data, and testing its accuracy on the remaining partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(training_data, k, training_model):\n",
    "    \"\"\"\n",
    "    Performs cross validation on the data set \n",
    "\n",
    "    :param training_data: the data set that is used to train the classifier\n",
    "    :param k: the number of partitions that the data set should be split in to\n",
    "    :param training_model: the sklearn model to train\n",
    "\n",
    "    :return accuracy: the mean accuracy of the model to 3 s.f.\n",
    "    \"\"\"\n",
    "    training = np.copy(training_data)\n",
    "    \n",
    "    #Randomise the training data\n",
    "    np.random.shuffle(training)\n",
    "    \n",
    "    #Splits the training data into k pieces\n",
    "    partitions = np.split(training, k)\n",
    "    \n",
    "    accuracy = 0\n",
    "    for i in range(k):\n",
    "        #Removes the given subarray from the list\n",
    "        data = np.delete(partitions, i, 0)\n",
    "        \n",
    "        #Trains the model on remaining training data (k-1 bits)\n",
    "        model = train(data.reshape((data.shape[1] * (k-1)),data.shape[2]), training_model)\n",
    "        predictions = test(partitions[i][:,1:], model)\n",
    "        \n",
    "        #Calculates the accuracy for\n",
    "        successes = np.equal(predictions, partitions[i][:,0])\n",
    "        successes[successes[:] == True].size\n",
    "        acc = successes[successes[:] == True].size / successes.size\n",
    "        accuracy += acc\n",
    "        \n",
    "    #Takes the mean accuracy over k samples\n",
    "    return round(accuracy / k, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN accuracy: 94.0%\n",
      "Logistic regression accuracy: 93.0%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "data = np.loadtxt(open(\"data/training_data_part_2.csv\"), delimiter=\",\")\n",
    "\n",
    "#Get the accuracy of knn\n",
    "knn_accuracy = cross_validation(data, 10, KNeighborsClassifier())\n",
    "print(\"KNN accuracy:\", str(knn_accuracy * 100) + \"%\")\n",
    "\n",
    "#Get the accuracy of logistic regression\n",
    "regression_accuracy = cross_validation(data, 10, LogisticRegression(solver='liblinear'))\n",
    "print(\"Logistic regression accuracy:\", str(regression_accuracy * 100) + \"%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
